---
layout: post
title: Hashing
permalink: /ds/hashing
---

- TOC
{:toc}

---

# Symbol Table

- Real time examples for dictionaries
  - Spell checker
  - The data dictionary found in database management applications
  - Symbol tables generated by loaders, assemblers, and compilers
  - Routing tables in networking components (DNS lookup)
- In computer science, we use the term ‘symbol table’ rather than ‘dictionary’ when referring to the abstract data type (ADT).

**What are Symbol Tables?**
- data structure that associates a value with a key.
- It supports the following operations:
  - Search whether a particular name is in the table
  - Get the attributes of that name
  - Modify the attributes of that name
  - Insert a new name and its attributes
  - Delete a name and its attributes
- There are only three basic operations on symbol tables: searching, inserting, and deleting.

![]({{site.cdn}}/cse/ds/hashing/symbol-table/dns-lookup.png)

![]({{site.cdn}}/cse/ds/hashing/symbol-table/symbol-table-implementations.png)

# Hashing

**What is Hashing?**
- Hashing is a technique used for storing and retrieving information as quickly as possible. 
- It is used to perform optimal searches and is useful in implementing symbol tables.

**Why Hashing?**
- Balanced BST - insert, delete and search in O(logn)
- Hashing does these operations in O(1), but elements are unordered..
  - Worst case complexity is O(n), but average O(1).
- BST easier to implement as hash functions can sometimes be very complex to generate.
- In BST, we can also efficiently find floor and ceil of values.

**Use Cases**
- can be used to remove duplicates from a set of elements, find frequency of all items.
- example: 
    - in web browsers, can check visited urls using hashing. 
    - in firewalls, can use hashing to detect spam by hashing IP addresses.
    - hashing can be used in any situation where want search() insert() and delete() in O(1) time.

**HashTable ADT**
- *CreatHashTable*: Creates a new hash table
- *HashSearch*: Searches the key in hash table
- *Hashlnsert*: Inserts a new key into hash table
- *HashDelete*: Deletes a key from hash table
- *DeleteHashTable*: Deletes the hash table

## How it works

**Print the first repeated character**
- Brute Force - Take each character and compare with all other chars to find if duplicate.
  - O(n2) - time complexity
  - O(1) - space complexity
- Better Solution - remember the previous characters in some array
- In simple terms we can treat array as a hash table.
  - number of possible characters is 256 (for simplicity assume ASCII characters only).
  - For each of the input characters go to the corresponding position and increment its count.

```java
int count[] = new int[256];
for(int i=0; i<str.length i++){
	if(count[str[i]] == 1){
		System.out.println(str[i]);
		break;
	} else{
		count[str[i]]++;
	}
}
System.out.println("No repeated characters");
```

**Why HashTable, Why not Arrays?**
- In the previous problem, we know the number of different possible characters (256) in advance.
- Suppose the given array has numbers instead of characters, set of possible values is infinity (or at least very big).
- The process of mapping the keys to locations is called ***hashing***.
- simple function is `key % table size`.

![]({{site.cdn}}/cse/ds/hashing/why-hashing.png)

# Components of Hashing

## Hash Table
- **Direct Addressing**
  - Hash table is a generalization of array.
  - Direct addressing is applicable when we can afford to allocate an array with one position for every possible key.
  - we find the element whose key is k by just looking in the kth position of the array. 
- **Hash Table**
  - If we have less locations and more possible keys, then simple array implementation is not enough.
  - Hash table or hash map is a data structure that stores the keys and their associated values
    - hash table uses a hash function to map keys to their associated values.
  - An array storing pointers to records with a specific hashcode value.
    - An entry in hash table is NIL if no existing hashcode has value equal to the index.

## Hash Function
- Used to transform the key into the index.
- **perfect hash function** - maps each item into a unique slot.
- Example - if the elements were nine-digit Social Security numbers, this method would require almost one billion slots. If we only want to store data for a class of 25 students, we will be wasting an enormous amount of memory.
- A good hash function should have following properties:
- **folding method** for constructing hash functions
  - begins by dividing the elements into equalsize pieces (the last piece may not be of equal size).
  - example - phone number element 436-555-4601 is taken in group of 2 (43,65,55,46,01). After the addition, 43+65+55+46+01, we get 210. Then perform modulo hash function.

**Characteristics of Good Hash Functions**
- Distribute key values evenly in the hash table
- Be easy and quick to compute, return value within range of locations in hash table.
- Minimize collision
- Use all the information provided in the key
- efficient collision resolution algorithm to compute an alternative index for a key.
- Have a high load factor for a given set of keys

## Load Factor
-  decision parameter used when we want to rehash or expand the existing hash table entries.
-  `load factor = number of items stored / size of the table`
-   helps us in determining the efficiency of the hashing function.
    -   It tells whether the hash function is distributing the keys uniformly or not.

## Collision
- condition where two records are stored in the same location.
- hash function gives same output for 2 different keys.

# Collision Resolution Techniques
- process of finding an alternate location is called collision resolution.
- **Direct Chaining** : An array of linked list application
  - Separate chaining
- **Open Addressing** : Array-based implementation
  - Linear probing (linear search)
  - Quadratic probing (nonlinear search)
  - Double hashing (use two hash functions)

## Direct Chaining
### Separate chaining

- combines linked representation with hash table.
- When two or more records hash to the same location, these records are constituted into a singly-linked list called a **chain**.

![]({{site.cdn}}/cse/ds/hashing/separate-chaining.png)

## Open Addressing
### Linear probing

- interval between probes is fixed at 1.
- we search the hash table sequentially,starting from the original hash location.
  - If a location is occupied, we check the next location.
  - We wrap around from the last table location to the first table location if necessary. 
- `rehash(key) = (n+1)%tableSize`
- table contains groups of consecutively occupied locations that are called clustering. 
  - one part of the table might be quite dense, even though another part has relatively few items.
- The step-size should be relatively prime to the table size.
  - If we choose the table size to be a prime number, then any step-size is relatively prime to the table size.
  - Clustering cannot be avoided by larger step-sizes.

### Quadratic probing

- The problem of Clustering can be reduced if we use the quadratic probing method.
- we start from the original hash location i.
  - If a location is occupied, we check the locations i + 12 , i +22, i + 32, i + 42...
  - We wrap around from the last table location to the first table location if necessary.
- `rehash(key) = (n+k^2) % tableSize`
- Even though clustering is avoided by quadratic probing, still there are chances of clustering.
- Both linear and quadratic probing use a probing sequence that is independent of the search key.

![]({{site.cdn}}/cse/ds/hashing/quadratic-probing.png)

### Double hashing

- Double hashing reduces clustering in a better way. 
- The increments for the probing sequence are computed by using a second hash function.
- If the location is occupied, we probe the location h1(key) + h2(key), h1(key) + 2 * h2(key), 

![]({{site.cdn}}/cse/ds/hashing/double-hashing.png)

## Comparisons: Open Addressing methods

|Linear Probing|Quadratic Probing|Double Hashing|
|---|---|---|
|Fastest among 3|Easiest to implement and deploy|Makes more efficient use of memory|
|Uses Few Probes|Uses extra memory, doesn't probe all locations in hash table|Uses fewer probes but takes more time.|
|Problem - Primary Clustering|Problem - Secondary Clustering|More complicated to implement|
|Fixed probe interval = 1|Interval between probes increases quadratically.|Interval between probes calculated ny another hash function|

# How Hashing gets O(1) Complexity

**how hashing gets O(1) if multiple elements map to the same location?**
- By using the load factor we make sure that each block (for example, linked list in separate chaining approach) on the average stores the maximum number of elements less than the load factor.
  - In practice this load factor is a constant (generally, 10 or 20).
  - As a result, searching in 20 elements or 10 elements becomes constant.
- If the average number of elements in a block is greater than the load factor, we rehash the elements with a bigger hash table size.
- The access time of the table depends on the load factor which in turn depends on the hash function.
- we generally use hash tables in cases where searches are more than insertion and deletion operations.

# Hashing Techniques

1. **Static Hashing**
   - When data is fixed.
   - Set of keys is kept fixed and given in advance, and the number of primary pages in the directory are kept fixed.
   - example - character array
2. **Dynamic Hashing**
   - When the data is not fixed.
   - set of keys can change dynamically.
   - example - numberic array

# Where not to use Hashing

- Problems for which data ordering is required
- Problems having multidimensional data
- Prefix searching, especially if the keys are long and of variable-lengths
- Problems that have dynamic data
- Problems in which the data does not have unique keys.

# Bloom Filters

- A probabilistic data structure which was designed to check whether an element is present in a set with memory and time efficiency.
- It tells us that the element either
  - definitely is not in the set, or
  - may be in the set.
- The base data structure of a Bloom filter is a Bit Vector.
- The algorithm relies on the use of a number of different hash functions.

**How it works?**

- apply k different hash functions to set bits.
- Search
  - apply the k hash functions and look up the indicated array elements.
    - If any of them are 0 we can be 100% sure that we have never encountered the element before.
    - Even if all of them are one, we still can’t conclude that we have seen the element.
      - All of the bits could have been set by the k hash functions applied to multiple other elements.
      - All we can conclude is that it is likely that we have encountered the element before.
- ***It is not possible to remove an element from a Bloom filter.***
  - We can’t unset a bit that appears to belong to an element because it might also be set by another element.
- If the bit array is mostly empty, i.e., set to zero, and the k hash functions are independent of one another, then the probability of a false positive is low.
  - As the bit array fills up, the probability of a false positive slowly increases.
- **One-time removal of an element from a Bloom filter can be simulated by having a second Bloom filter that contains elements that have been removed.**
  - false positives in the second filter become false negatives in the composite filter, which may be undesirable.
  - In this approach, readding a previously removed item is not possible, as one would have to remove it from the removed filter.

![]({{site.cdn}}/cse/ds/hashing/bloom-filters.png)

**Selecting hash functions**
- k different independent hash functions.
- For a good hash function with a wide output, there should be little if any correlation between different bit-fields of such a hash, so this type of hash can be used to generate multiple different hash functions by slicing its output into multiple bit fields. 
- Alternatively, one can pass k different initial values (such as 0, 1, ..., k - 1) to a hash function that takes an initial value - or add (or append) these values to the key.

**Selecting size of bit vector**
- A Bloom filter with **1% error** and an optimal value of k, in contrast, requires only about **9.6 bits per element** – regardless of the size of the elements. 
- The 1% false positive rate can be reduced by a factor of ten by adding only about 4.8 bits per element.

**Space Advantages**
- Bloom filters have a strong space advantage over other data structures for representing sets, such as self-balancing binary search trees, tries, hash tables, or simple arrays or linked lists of the entries.
- No elements, no pointers here.

**Time Advantages**
-  The time needed either to add items or to check whether an item is in the set is a fixed constant, O(k), completely independent of the number of items already in the set.
-  Average access time of sparse hash tables can make them faster in practice than some Bloom filters.
   -  In a hardware implementation, however, the Bloom filter shines because its k lookups are independent and can be parallelized.

# Hashing Questions
## Remove Duplicate from character array

### Brute Force
- Time - O(n2)
- Space - O(1)
- Take each character and compare with all the later characters

```java
public static void main(String[] args) {
    char str[] = "geeksforgeeks".toCharArray();
    int n = str.length;
    System.out.println(removeDuplicate(str, n));
}

static String removeDuplicate(char str[], int n) {
    int index = 0;
    for (int i = 0; i < n; i++) {
        int j;
        for (j = 0; j < i; j++)
            if (str[i] == str[j])
                break;
        // If not present, then add it to result. 
        if (j == i)
            str[index++] = str[i];

    }
    return String.valueOf(Arrays.copyOf(str, index));
}
```
```
geksfor
```

### Sort to bring repeated characters
- Time - O(n logn)
- Space - O(1)

```java
public static void main(String[] args) {
    char str[] = "geeksforgeeks".toCharArray();
    Arrays.sort(str);
    int n = str.length;
    System.out.println(removeDuplicate(str, n));
}

static String removeDuplicate(char str[], int n) {
    int index = 0;
    for (int i = 1; i < n; i++) {
        if (str[i] == str[index]) {
            continue;
        }
        str[++index] = str[i];
    }
    return String.valueOf(Arrays.copyOf(str, index + 1));
}
```
```
[e, e, e, e, f, g, g, k, k, o, r, s, s]
[e, f, g, k, o, r, s, k, k, o, r, s, s]
efgkors
```

### Use HashSet (Internally uses HashMap) - Hashing
- Time - O(n)
- Space - O(n)

```java
public static void main(String[] args) {
    char str[] = "geeksforgeeks".toCharArray();
    int n = str.length;
    System.out.println(removeDuplicate(str, n));
}

static String removeDuplicate(char str[], int n) {
    Set<Character> hashset = new HashSet<>();
    int index = 0;
    for (int i = 0; i < n; i++) {
        if (hashset.contains(str[i])) {
            continue;
        } else {
            hashset.add(str[i]);
            str[index++] = str[i];
        }
    }
    return String.valueOf(Arrays.copyOf(str, index));
}
```

## Check 2 Unordered Arrays

```
A = {2,5,6,8,10,2,2}
B = {2,5,5,8,10,5,6}
```

### Solution 1
- For each element in A, check element is in B or not.
- Problem - Duplicates (number of occurences)

### Solution 2 (Sorting)
- Sort and iterate over both to check the elements
- Time - O(n logn) ~ O(n logn) + O(n)

### Solution 3 (Hashing)
- Create frequency map for A.
- For each element in B, decrease frequency.
- Map should be empty.

## Report all pairs

```
Input - { {1,3},{2,6},{3,5},{7,4},{5,3},{8,7}}
Output - {3,5} and {5,3}
```

- With hashing, can be solved in single scan
- Insert each pair as key, value in hashMap.
- While insert check if value is present as existing key and key is corresponding value in the entry.

## Two Sets A and B, pair for sum K?

- Create hashSet for A.
- Iterate over B, check hashSet for K-element.
- Time - O(Max(m,n))
- Space - O(Min(m,n))

## First repeated or non-repeated character in string
- use frequency map (hashmap)

## Million of lines, only 2 lines are identical

**Statement**  
We have a file with millions of lines of data. Only two lines are identical; the rest are unique. Each line is so long that it may not even fit in the memory. What is the most efficient solution for finding the identical lines?

**Solution**
- Since a complete line may not fit into the main memory, read the line partially and compute the hash from that partial line.
- Then read the next part of the line and compute the hash. This time use the previous hash also while computing the new hash value.
- Continue this process until we find the hash for the complete line.
- Do this for each line and store all the hash values in a file [or maintain a hash table of these hashes].
- If at any point you get same hash value, read the corresponding lines part by part and compare.
